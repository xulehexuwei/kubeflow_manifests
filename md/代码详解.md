## 梯度优化optimizer

```python
for epoch in range(100):
   for batch_idx, (data, target) in enumerate(train_loader):
       images = images.cuda(non_blocking=True)
       target = target.cuda(non_blocking=True)
       ...
       output = model(images)
       loss = criterion(output, target)
       ...
       optimizer.zero_grad()
       loss.backward()
       optimizer.step()
```

详解如下代码

```python
output = model(images)
loss = criterion(output, target)
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

- outputs = net(inputs) 即前向传播求出预测的值

- loss = criterion(outputs, labels) 这一步很明显，就是求loss

- optimizer.zero_grad() 意思是把梯度置零，也就是把loss关于weight的导数变成0.

- loss.backward() 即反向传播求梯度

- optimizer.step() 即更新所有参数，step()函数的作用是执行一次优化步骤，通过梯度下降法来更新参数的值。因为梯度下降是基于梯度的，所以在执行optimizer.step()函数前应先执行loss.backward()函数来计算梯度。

注意：optimizer只负责通过梯度下降进行优化，而不负责产生梯度，梯度是tensor.backward()方法产生的。

